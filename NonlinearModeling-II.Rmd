---
title: "Nonlinear Modeling II"
author: "M Loecher"
output:
   pdf_document:
     extra_dependencies: ["pdfpages"]
     toc: true
     toc_depth: 2
     includes:
        in_header: header.tex
classoption: landscape
fontsize: 16pt
header-includes: \usepackage{amsmath}
subtitle: Generalized Additive Models (close to chapter 7, ISLR)
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)

#library(gamair)
data(mpg, package="gamair")
library(lubridate)
library(knitr)
library(mgcv)
library(xts)
library(ISLR)
data(Auto)
data(mcycle,package="MASS")
library(splines)
library(boot)
library(mgcv)
library(kableExtra)

```


\fontsize{16}{22}
\fontseries{b}
\selectfont


\includepdf[pages={2}]{../datacamp/chapter1.pdf}

# Pros and Cons of GAMs

\benum

\item[+] GAMs allow us to fit a non-linear $f_j$ to each $X_j$, so that we can
automatically model non-linear relationships that standard linear regression
will miss. This means that we do not need to manually try
out many different transformations on each variable individually.
\item[+] The non-linear fits can potentially make more accurate predictions
for the response Y .
\item[+] Because the model is additive, we can still examine the effect of
each $X_j$ on Y individually while holding all of the other variables
fixed. Hence if we are interested in inference, GAMs provide a useful
representation.

\item[+] The smoothness of the function $f_j$ for the variable $X_j$ can be summarized
via degrees of freedom.

\item[-] The main limitation of GAMs is that the model is restricted to be
additive. With many variables, important interactions can be missed.
However, as with linear regression, we can manually add interaction
terms to the GAM model by including additional predictors of the
form $X_j × X_k$. In addition we can add low-dimensional interaction
functions of the form $f_{jk}(X_j,X_k)$ into the model; such terms can
be fit using two-dimensional smoothers such as local regression, or
two-dimensional splines (not covered here).

\eenum

```{r echo=FALSE}
set.seed(1)
N=250
x = seq(0,1,length=N)
y = 10 - 6*x + (8-7*x)*sin(4*pi*x) + rnorm(N,s=2)
#fit = gam(y~s(x,k=10))
#summary(fit)
```

### Syntax I

```{r}
fit = gam(y~s(x,sp=0.1))
```



Choosing the Right Smoothing Parameter (Remember Goldilocks and the three bears?)


```{r, fig.width=10, echo=FALSE}
par(mfrow=c(1,3))
#plot(y~x,pch=20,col="darkgray");grid()
lambda = c(100,0.00001,0.01)
lambdaIs = c("too large","too small","just right")
for (i in 1:3){
  l = lambda[i]
  if (l<0.01) k=80 else k = 10
  fit = gam(y~s(x,sp=l,k=k))
  plot(fit, shade = TRUE, residuals = FALSE, shift = coef(fit)[1], rug=FALSE,shade.col=rgb(0,0,0.8,0.2),lwd=2.5,col="purple",ylab="",main=bquote(lambda ~ .(lambdaIs[i])), cex.main=2);grid()
  points(x,y,pch=20, cex=2,col="darkgray")  
}

```

### Syntax II

```{r}
fit = gam(y~s(x,k=4))
```

\vspace{-0.75in}
\includegraphics[width=8in]{figures/NumBasisFunctions.pdf}

## [GAM Exercises part I](exercises/GAM-Exercises-I.html)

# Multiple Smooths

A true understanding of the label *additive* only comes in the context of multiple variables. A GAM is an extension of the multiple linear regression model which allows for non-linear relationships between each feature and the response. We simply replace each linear component $\beta_j x_{ij}$ with a (smooth) nonlinear function $f_j( x_{ij} )$ so that the model becomes:

$$
\label{eq:gamDef}
y_i = \beta_0 + f_1( x_{i1} ) + f_2( x_{i2} ) \ldots + f_p( x_{ip} ) + \epsilon_i = \beta_0 + \sum_{j=1}^p{f_j( x_{ij} )}  + \epsilon_i
$$

## Our Working Dataset: mpg

```{r, results = 'asis',echo=FALSE}
#knitr::kable(head(mpg[,-c(1:2)]), format = "latex")
knitr::kable(head(mpg[,-c(1:2)]), "latex", booktabs = T) %>%
kable_styling(latex_options = "striped")
```


### Syntax III


```{r}
mod2 <- gam(hw.mpg ~ s(weight) + s(length) , data = mpg, method = "REML")
```

```{r, fig.width=10, echo=FALSE}
plot(mod2, page = 1, shade = TRUE, residuals = TRUE, cex=1.2,shift = coef(mod2)[1], rug=FALSE)
```

### Linear Terms I

```{r,eval=FALSE}
mod3 <- gam(hw.mpg ~ s(weight) + length , data = mpg, method = "REML")
```

```{r, fig.width=10, echo=FALSE}
#mod3 <- gam(hw.mpg ~ s(weight) + s(length,k=0,fx=TRUE) , data = mpg, method = "REML")
mod3 <- gam(hw.mpg ~ s(weight) + s(length,sp=1000) , data = mpg, method = "REML")
plot(mod3, page = 1, shade = TRUE, residuals = TRUE, cex=1.2,shift = coef(mod3)[1], rug=FALSE)
```
### Linear Terms II

```{r,eval=TRUE}
mod4 <- gam(hw.mpg ~ s(weight) + s(length,sp=100) , data = mpg, method = "REML")
```

```{r, fig.width=10, echo=FALSE}
#mod3 <- gam(hw.mpg ~ s(weight) + s(length,k=0,fx=TRUE) , data = mpg, method = "REML")

plot(mod4, page = 1, shade = TRUE, residuals = TRUE, cex=1.2,shift = coef(mod4)[1], rug=FALSE)
```
### Categorical Terms I

```{r,eval=TRUE}
mod5 <- gam(hw.mpg ~ s(weight) + fuel , data = mpg, method = "REML")
summary(mod5)
```

```{r, fig.width=10, echo=FALSE}
#mod3 <- gam(hw.mpg ~ s(weight) + s(length,k=0,fx=TRUE) , data = mpg, method = "REML")

plot(mod5, page = 1, shade = TRUE, residuals = TRUE,pch=1, cex=1,shift = coef(mod5)[1], rug=FALSE, all.terms = TRUE)
```

\LARGE

In this model the nonlinear effects of weight is the same for both fuel types. (You coulod view this as the analogy of the fixed slope, different intercepts in linear models!)



### Categorical Terms II

We can also specify different smooths for different levels of the categorical variable! ("a factor smooth interaction")

```{r,eval=TRUE}
mod6 <- gam(hw.mpg ~ s(weight, by= fuel) + fuel, data = mpg, method = "REML")
summary(mod6)
```

```{r, fig.width=10, echo=FALSE}
#mod3 <- gam(hw.mpg ~ s(weight) + s(length,k=0,fx=TRUE) , data = mpg, method = "REML")

plot(mod6, page = 1, shade = TRUE, residuals = TRUE, cex=2,shift = coef(mod6)[1], rug=FALSE)
```

# Interactions

Remember the Advertising Data from the ISLR book:


```{r}
Advertising <- read.csv("data/Advertising.csv")
#get rid of the first column
Advertising <- Advertising[,-1]

```

```{r, fig.width=10, echo=FALSE}
plotAds = function(x="TV", myData=Advertising, col = "blue",m=m, LS=TRUE){
  plot(myData[,x], myData$Sales, pch=20, col = col, xlab = x, ylab = "Sales");grid();
  title(paste("r=", round(cor(myData[,x], myData$Sales),2)))
  m = colMeans(Advertising[,c(x,"Sales")])
  points(m[x],m["Sales"], col="red",pch=19,cex=1.75)
  if (LS) {
    abline(lm.fit(cbind(rep(1,nrow(myData)),myData[,x]), myData$Sales), col=2, lwd=2)
  }
}

par(mfrow=c(1,3),cex=1)
plotAds("TV")
plotAds("Radio", col = "darkgreen")
plotAds("Newspaper", col = "purple")
```

#### Linear Model First !

```{r}
fit = lm(Sales ~ Newspaper + TV + Radio, data = Advertising)
summary(fit)
```
#### Repeat with GAM

```{r}
gamAd = gam(Sales ~ s(Newspaper) + s(TV) + s(Radio), data = Advertising)
summary(gamAd)
```

```{r}
plot(gamAd)
```

\begin{figure}
\includegraphics[width=8in]{figures/ISLR-Fig-3-5.pdf}
\caption{FIGURE 3.5. For the Advertising data, a linear regression fit to sales using
TV and radio as predictors. From the pattern of the residuals, we can see that
there is a pronounced non-linear relationship in the data. The positive residuals
(those visible above the surface), tend to lie along the 45-degree line, where TV
and Radio budgets are split evenly. The negative residuals (most not visible), tend
to lie away from this line, where budgets are more lopsided.}
\end{figure}

```{r}
fit2 = lm(Sales ~  TV*Radio, data = Advertising)
summary(fit2)
```

#### Interactions in GAMs


```{r}
gamAd2 = gam(Sales ~ s(Radio) + s(TV) + s(Radio, TV), data = Advertising)
summary(gamAd2)
```

```{r,fig.width=12}
par(mfrow=c(1,2),mar=c(1,1,1,1))
vis.gam(gamAd2)
vis.gam(gamAd2, n.grid = 50, theta = 35, phi = 32, zlab = "",
        ticktype = "detailed", color = "topo")
```


## Classification

Let us make a binary feature in the Advertising data:

```{r}
Advertising$HighSales = Advertising$Sales > median(Advertising$Sales)
Advertising$HighSales = as.numeric(Advertising$HighSales)

```

```{r}
gamAd3 = gam(HighSales ~ s(Radio) + s(TV), data = Advertising, type ="binomial")
summary(gamAd3)
plot(gamAd3)
```



## [GAM Exercises part II](exercises/GAM-Exercises-II.html)

# Interpreting GAM outputs

```{r}
mod_city4 <- gam(hw.mpg ~ s(weight) +  s(price) + s(rpm) + s(comp.ratio) + s(width) 
                 + fuel , data = mpg, method = "REML")
summary(mod_city4)

```

#### Effective Degrees of Freedom

```{r, fig.width=10, echo=FALSE}
par(mfrow=c(1,2))
plot(mod_city4, select = 1, shade = TRUE, residuals = TRUE, cex=1.2,shift = coef(mod_city4)[1], rug=FALSE)
plot(mod_city4, select = 4, shade = TRUE, residuals = TRUE, cex=1.2,shift = coef(mod_city4)[1], rug=FALSE)
```


#### Significance of Smooth Terms

Overall significance of smooth. p-values are approximate.

One way to interpret significance: you cannot draw a horizontal line through the 95% confidence interval.

```{r, fig.width=10, echo=FALSE}
par(mfrow=c(1,2))
plot(mod_city4, select = 1, shade = TRUE, residuals = TRUE, cex=1.2,shift = coef(mod_city4)[1], rug=FALSE)
abline(h= coef(mod_city4)[1],col=2)
plot(mod_city4, select = 2, shade = TRUE, residuals = TRUE, cex=1.2,shift = coef(mod_city4)[1], rug=FALSE)
abline(h= coef(mod_city4)[1],col=2)
```

#### Significance and Effective Degress of Freedom

Two separate concepts, one does not imply the other !

```{r, results = 'asis',echo=FALSE}
knitr::kable(summary(mod_city4)$s.table, format = "latex")
```

```{r, fig.width=10, echo=FALSE}
par(mfrow=c(1,3))
plot(mod_city4, select = 2, shade = TRUE, residuals = TRUE, cex=1.2,shift = coef(mod_city4)[1], rug=FALSE)
abline(h= coef(mod_city4)[1],col=2)
plot(mod_city4, select = 3, shade = TRUE, residuals = TRUE, cex=1.2,shift = coef(mod_city4)[1], rug=FALSE)
abline(h= coef(mod_city4)[1],col=2)
plot(mod_city4, select = 4, shade = TRUE, residuals = TRUE, cex=1.2,shift = coef(mod_city4)[1], rug=FALSE)
abline(h= coef(mod_city4)[1],col=2)
```



# Visualizing GAMs

```{r}
gam_mod <- gam(hw.mpg ~ s(weight) +  s(rpm) + s(price) + s(comp.ratio) , data = mpg, method = "REML")
```


```{r, fig.width=10}
plot(gam_mod, pages=1, rug=FALSE)
```


```{r, fig.width=10}
par(mfrow=c(2,2))
plot(gam_mod, select = 1, shade = TRUE, residuals = TRUE)
plot(gam_mod, select = 1, shade = TRUE, residuals = TRUE, shade.col ="lightblue")
plot(gam_mod, select = 4, shade = TRUE, residuals = TRUE, shade.col ="lightblue", rug=FALSE,pch=1)
plot(gam_mod, select = 4, shade = TRUE, residuals = TRUE, shade.col ="lightgreen", rug=FALSE,shift = coef(gam_mod)[1])
```


```{r}
plot(mod_city4, select = 1, shade = TRUE, residuals = TRUE, cex=1.2,shift = coef(mod_city4)[1], rug=FALSE)
```

# Model checking, Concurvity

\includepdf[pages={26-31,34-37}]{../datacamp/chapter2.pdf}



# The gam library

In order to fit more general sorts of GAMs, using smoothing splines
or other components that cannot be expressed in terms of basis functions
and then fit using least squares regression, we will need to use the gam
library in R.

## Smoothing/number of knots specified by user

A "simpler" version of mgcv. (no Bayesian smoothing)
Note the slightly different syntax though (e.g., *df=4*, instead of *k=4*) !!

The `s()` function, which is part of the gam library, is used to indicate that
we would like to use a smoothing spline. We specify that the function of
year should have 4 degrees of freedom, and that the function of *age* will
have 5 degrees of freedom. Since *education* is qualitative, we leave it as is,
and it is converted into four dummy variables. We use the `gam()` function in
order to fit a GAM using these components. 

```{r, fig.width=10,out.width='.95\\linewidth'}
gam1 <- lm(wage ~ ns(year, 4) + ns(age, 5) + education, data = Wage)

detach("package:mgcv", unload=TRUE)#avoid name space collision!
library(gam)
gam.m3 <- gam(wage ~ s(year, 4) + s(age, 5) + education, data = Wage)
par(mfrow = c(1, 3));plot(gam.m3, se = TRUE, col = "blue")

```

Conveniently, even though gam1 is not of class gam but rather of class `lm`, we can still use `plot.gam()` on it. 

```{r, fig.width=10,out.width='.95\\linewidth'}
par(mfrow = c(1, 3));plot.Gam(gam1, se = TRUE, col = "red")
```

In these plots, the function of year looks rather linear. We can perform a
series of ANOVA tests in order to determine which of these three models is
best: a GAM that excludes year (M1), a GAM that uses a linear function
of year (M2), or a GAM that uses a spline function of year (M3).

```{r}
gam.m1 <- gam(wage ~ s(age, 5) + education, data = Wage)
gam.m2 <- gam(wage~ year + s(age, 5) + education, data = Wage)
anova(gam.m1, gam.m2, gam.m3, test = "F")
```

We find that there is compelling evidence that a GAM with a linear function
of year is better than a GAM that does not include year at all
(p-value=0.00014). However, there is no evidence that a non-linear function
of year is needed (p-value=0.349). In other words, based on the results
of this ANOVA, M2 is preferred.
The summary() function produces a summary of the gam fit.

```{r}
summary(gam.m3)
```

The p-values for *year* and *age* correspond to a null hypothesis of a linear
relationship versus the alternative of a non-linear relationship. The large
p-value for *year* reinforces our conclusion from the ANOVA test that a linear
function is adequate for this term. However, there is very clear evidence
that a non-linear term is required for *age*.
We can make predictions from *gam* objects, just like from lm objects,
using the predict() method for the class *gam*. Here we make predictions on
the training set.

```{r}
preds <- predict(gam.m2, newdata = Wage)
```

### Logistic regression

In order to fit a logistic regression GAM, we once again use the `I()` function
in constructing the binary response variable, and set `family=binomial`.


```{r, fig.width=10,out.width='.95\\linewidth'}
gam.lr.s <- gam(I(wage > 250) ~ year + s(age, df = 5) + education,
                family = binomial, data = Wage, 
                subset = (education != "1. < HS Grad"))
par(mfrow = c(1, 3)); plot(gam.lr.s, se = TRUE, col = "green")
```

# Exercises

\benum

\item \textbf{Exercises, Coding}

This question relates to the "College" data set.

\benum

\item Split the data into a training set and a test set. Using out-of-state tuition as the response and the other variables as the predictors, perform forward stepwise selection on the training set in order to identify a satisfactory model that uses just a subset of the predictors.

```{r, echo = TRUE, fig.width=10,out.width='.95\\linewidth'}
library(leaps)
set.seed(1)
attach(College)
train <- sample(length(Outstate), length(Outstate) / 2)
test <- -train
College.train <- College[train, ]
College.test <- College[test, ]
fit <- regsubsets(Outstate ~ ., data = College.train, nvmax = 17, method = "forward")
fit.summary <- summary(fit)
par(mfrow = c(1, 3))
plot(fit.summary$cp, xlab = "Number of variables", ylab = "Cp", type = "l")
min.cp <- min(fit.summary$cp)
std.cp <- sd(fit.summary$cp)
abline(h = min.cp + 0.2 * std.cp, col = "red", lty = 2)
abline(h = min.cp - 0.2 * std.cp, col = "red", lty = 2)
plot(fit.summary$bic, xlab = "Number of variables", ylab = "BIC", type='l')
min.bic <- min(fit.summary$bic)
std.bic <- sd(fit.summary$bic)
abline(h = min.bic + 0.2 * std.bic, col = "red", lty = 2)
abline(h = min.bic - 0.2 * std.bic, col = "red", lty = 2)
plot(fit.summary$adjr2, xlab = "Number of variables", ylab = "Adjusted R2", type = "l", ylim = c(0.4, 0.84))
max.adjr2 <- max(fit.summary$adjr2)
std.adjr2 <- sd(fit.summary$adjr2)
abline(h = max.adjr2 + 0.2 * std.adjr2, col = "red", lty = 2)
abline(h = max.adjr2 - 0.2 * std.adjr2, col = "red", lty = 2)
```

*Cp, BIC and adjr2 show that size 6 is the minimum size for the subset for which the scores are within 0.2 standard devitations of optimum.*

```{r, echo = TRUE}
fit <- regsubsets(Outstate ~ ., data = College, method = "forward")
coeffs <- coef(fit, id = 6)
names(coeffs)
```

\item Fit a GAM on the training data, using out-of-state tuition as the response and the features selected in the previous step as the predictors. Plot the results, and explain your findings.

\item Evaluate the model obtained on the test set, and explain the results obtained.

\item For which variables, if any, is there evidence of a non-linear relationship with the response ?

\eenum


\eenum

## [GAM Exercises part III](exercises/GAM-Exercises-CrimeData.html)

# Autocorrelations

GAM or MLR have assumptions in a model that errors (residuals) are identically and independently distributed (i.i.d.). In the case of the time series regression, it is very strong assumption, which is here, logically, not fulfilled. Present time series values are highly correlated with past values, so errors of the model will be correlated too. This phenomenon is called an autocorrelation. This implies that estimated regression coefficients and residuals of a model might be negatively biased, which also implies that previously computed p-values of statistical tests or confidence intervals are wrong.

How can we handle this situation? By inclusion of autoregressive model (AR) for errors in our model. So we have the model with the term for errors like this:
$$
y_i = \beta \cdot X + \epsilon_i, \mbox{      } \epsilon_i = \phi \epsilon_{i-1} + v_i 
$$


where the second equation is a classical AR(1) process and $\phi$

is an unknown autoregressive coefficient to be estimated. Errors can be also nested within a week, which is in our case more appropriate, because of the double seasonal character of our time series. You can add also higher orders of AR process and also MA (moving average) model. You can read more about estimation and modeling of this kind of models in an excellent book by Box, Jenkins, and Reinsel: Time Series Analysis.

It’s possible to add correlation term for errors with function gamm, which stands for GAM mixture models. It calls the `lme` function from package `nlme`. 

```{r, echo=TRUE, fig.width=10, fig.height=8}
detach("package:gam", unload=TRUE)#avoid name space collision!
library(mgcv)
## now an example with autocorrelated errors....
n <- 200;sig <- 2
x <- 0:(n-1)/(n-1)
f <- 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10
e <- rnorm(n,0,sig)
for (i in 2:n) e[i] <- 0.6*e[i-1] + e[i]
y <- f + e
op <- par(mfrow=c(2,2))
## Fit model with AR1 residuals
b <- gamm(y~s(x,k=20),correlation=corAR1())
plot(b$gam);lines(x,f-mean(f),col=2)
## Raw residuals still show correlation, of course...
acf(residuals(b$gam),main="raw residual ACF")
## But standardized are now fine...
acf(residuals(b$lme,type="normalized"),main="standardized residual ACF")
## compare with model without AR component...
b <- gam(y~s(x,k=20))
plot(b);lines(x,f-mean(f),col=2)
```


\newpage
# Appendix I

## Locations of knots

For penalized regression spline, the exact locations are not important, as long as:

* k is adequately big;
* the spread of knots has good, reasonable coverage.

By default:

* natural cubic regression spline bs = 'cr' places knots by quantile;
* B-splines family (bs = 'bs', bs = 'ps', bs = 'ad') place knots evenly.

Compare the following:

```{r, fig.width=10,out.width='.95\\linewidth',fig.height=3}
library(mgcv)

## toy data
set.seed(0); x <- sort(rnorm(400, 0, pi))  ## note, my x are not uniformly sampled
set.seed(1); e <- rnorm(400, 0, 0.4)
y0 <- sin(x) + 0.2 * x + cos(abs(x))
y <- y0 + e

## fitting natural cubic spline
cr_fit <- gam(y ~ s(x, bs = 'cr', k = 20))
cr_knots <- cr_fit$smooth[[1]]$xp  ## extract knots locations

## fitting B-spline
bs_fit <- gam(y ~ s(x, bs = 'bs', k = 20))
bs_knots <- bs_fit$smooth[[1]]$knots  ## extract knots locations

## summary plot
par(mfrow = c(1,2))
plot(x, y, col= "grey", main = "natural cubic spline");
lines(x, cr_fit$linear.predictors, col = 2, lwd = 2)
abline(v = cr_knots, lty = 2)
plot(x, y, col= "grey", main = "B-spline");
lines(x, bs_fit$linear.predictors, col = 2, lwd = 2)
abline(v = bs_knots, lty = 2)
```


## Setting your own knots locations:

You can also provide your customized knots locations via the knots argument of gam() (yes, knots are not fed to s(), but to gam()). For example, you can do evenly spaced knots for cr:

```{r}
xlim <- range(x)  ## get range of x
myfit <- gam(y ~ s(x, bs = 'cr', k =20),
         knots = list(x = seq(xlim[1], xlim[2], length = 20)))
```


Now you can see that:

```{r,fig.height=3}
my_knots <- myfit$smooth[[1]]$xp
plot(x, y, col= "grey", main = "my knots");
lines(x, myfit$linear.predictors, col = 2, lwd = 2)
abline(v = my_knots, lty = 2)
```


# Appendix II, REML

REML stands for *REstricted Maximum Likelihood*.


The discussion below is relatively technical and readers may want to consult Simon Wood's excellent ["Generalized Additive Models: An Introduction with R"](https://www.crcpress.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331) while reading.

## Smoothing parameter estimation in `mgcv`

`mgcv` has a number of options for selecting the smoothness of terms in the model. Smoothness selection method is specified via the `method=` argument and has the following options: `"GCV.Cp"`, `"GACV.Cp"`, `"REML"`, `"P-REML"`, `"ML"` and `"P-ML"`. These methods can be separated into two groups: "prediction error" (`"GCV.Cp"` and `"GACV.Cp"`) and "likelihood" (`"REML"`, `"P-REML"`, `"ML"` and `"P-ML"`). Here we'll just address `"GCV.Cp"` vs. `"REML"` (though similar arguments can be levelled in favour of `"ML"` vs `"GCV.Cp"`. `mgcv` uses `"GCV.Cp"` as a default, though this is mainly for backwards compatibility (Simon Wood, pers. comm.).

## Theory

Both REML and GCV try to do the same thing: make the smooths in your model just wiggly enough and no wigglier. It has been shown that GCV will select optimal smoothing parameters (in the sense of low prediction error) when the sample size is infinite. In practice there is presumably some sample size at which this asymptotic result kicks in, but few of us have data of that size. At smaller (finite) sample sizes GCV can develop multiple minima making optimisation difficult and therefore tends to give more variable estimates of the smoothing parameter. GCV also tends to undersmooth (i.e., smooths are more wiggly than they should be), as it penalizes overfit weakly (which makes sense for a prediction error based critera, where you want your model to fit your data well). This was shown in work by Phil Reiss and Todd Ogden (2009) as well as Simon Wood (2011). They also showed that REML (and ML, marginal likelihood) penalize overfitting more and therefore have more prounounced optima, leading to fewer optimisation issues and less variable estimates of the smoothing parameter.

This is best illustrated in this figure from Wood (2011) which shows the (log) smoothing parameter vs. REML and GCV. The ticks on the horizontal axis show where the optimum smoothing parameter lies for each example data set (profile of the REML/GCV criteria shown as lines). The REML ticks are close together and the lines all look similar. For GCV this is not the case.

![gcv and reml scores as functions of the smoothing parameter](figures/gcvreml.png)

Section 1.1 of Wood (2011) gives an overview of these issues, though the rest of the paper is somewhat technical.

## Practice

In practice I've analysed multiple datasets using both REML and GCV and in some cases have seen very different results when it comes to model and term selection. If GCV is prone to undersmoothing at finite sample sizes, then we will end up fitting models that are more wiggly than we want. Overfitting is surely something we want to avoid. It is not clear how many datasets (or what particular type of data) is vulnerable to this issue, but we (Distance Development Team) thought it best to switch to REML by default to avoid potential overfitting and highly variable smoothing parameter estimates.


## Model comparison

Note that moving to REML, we need to be careful about model comparison. Models fitted with REML cannot be compared by their REML scores when their unpenalized components are different. In practice this means that models that use a shrinkage basis (e.g., `"ts"` or `"cs"`) for all terms in the model or that use `select=TRUE` are fine but other models are not.

If marginal likelihood (ML) is used, we can compare those scores.

One can compare AIC (extracted using the `AIC()` function in R) of models fitted with REML or ML (or GCV), though obviously this is not the only model selection step!


## References

- Reiss, P.T. and Ogden, R.T. (2009) Smoothing parameter selection for a class of semiparametric linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71, 505–523. [pdf](https://works.bepress.com/phil_reiss/1/)
- Wood, S.N. (2006) Generalized Additive Models. CRC Press.
- Wood, S.N. (2011) Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73, 3–36. [pdf](http://opus.bath.ac.uk/22707/1/Wood_JRSSB_2011_73_1_3.pdf)